        "import os\n",
        "import sys\n",
        "import time\n",
        "import subprocess\n",
        "import tempfile\n",
        "import wave\n",
        "import contextlib\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# Safe installer helper (opt-in)\n",
        "# --------------------\n",
        "\n",
        "def maybe_install_packages(packages: Optional[List[str]] = None, force: bool = False):\n",
        "    \"\"\"Install packages via pip **only** when explicitly requested.\n",
        "\n",
        "    Usage examples:\n",
        "      maybe_install_packages(['yt-dlp','pydub'], force=True)\n",
        "\n",
        "    Or set AUTO_INSTALL_DEPENDENCIES=1 in the environment and call\n",
        "      maybe_install_packages()\n",
        "\n",
        "    This prevents the SyntaxError that happens when `!pip` is placed inside\n",
        "    a Python module that is executed outside an interactive IPython cell.\n",
        "    \"\"\"\n",
        "    packages = packages or [\n",
        "        'yt-dlp', 'pydub', 'webrtcvad', 'scikit-learn', 'resemblyzer',\n",
        "        'faster-whisper', 'transformers', 'torch', 'sentencepiece'\n",
        "    ]\n",
        "    if not force and os.environ.get('AUTO_INSTALL_DEPENDENCIES', '0') != '1':\n",
        "        print('AUTO_INSTALL_DEPENDENCIES is not set. To auto-install, set AUTO_INSTALL_DEPENDENCIES=1 or call with force=True.')\n",
        "        return\n",
        "    cmd = [sys.executable, '-m', 'pip', 'install', '--upgrade'] + packages\n",
        "    print('Installing packages:', packages)\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# Audio I/O helpers (no libsndfile required)\n",
        "# --------------------\n",
        "\n",
        "def read_wav_via_wave(path: str) -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"Read a WAV file using Python's wave module and return (samples_float, sr).\n",
        "\n",
        "    The returned samples are floats in range [-1.0, 1.0] and mono.\n",
        "    This avoids dependency on the `soundfile` package and libsndfile.\n",
        "    \"\"\"\n",
        "    with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
        "        n_channels = wf.getnchannels()\n",
        "        sampwidth = wf.getsampwidth()\n",
        "        sr = wf.getframerate()\n",
        "        nframes = wf.getnframes()\n",
        "        raw = wf.readframes(nframes)\n",
        "    # Interpret raw bytes based on sample width\n",
        "    if sampwidth == 2:\n",
        "        dtype = np.int16\n",
        "    elif sampwidth == 4:\n",
        "        dtype = np.int32\n",
        "    elif sampwidth == 1:\n",
        "        # 8-bit WAVs are unsigned\n",
        "        dtype = np.uint8\n",
        "    else:\n",
        "        raise RuntimeError(f'Unsupported sample width: {sampwidth}')\n",
        "    audio = np.frombuffer(raw, dtype=dtype)\n",
        "    if sampwidth == 1:\n",
        "        # convert unsigned 8-bit to signed centered at zero\n",
        "        audio = (audio.astype(np.int16) - 128).astype(np.int16)\n",
        "    if n_channels > 1:\n",
        "        audio = audio.reshape(-1, n_channels)\n",
        "        audio = audio.mean(axis=1)\n",
        "    # normalize to float32\n",
        "    max_val = float(np.iinfo(dtype).max) if np.issubdtype(dtype, np.integer) else 1.0\n",
        "    audio = audio.astype(np.float32) / max_val\n",
        "    return audio, sr\n",
        "\n",
        "\n",
        "def load_audio_mono(path: str) -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"Load audio and return (samples_float_mono, sample_rate).\n",
        "\n",
        "    Prefer WAV via builtin `wave`; if the file is not WAV, try pydub (if\n",
        "    installed). If neither method works, raise an informative error.\n",
        "    \"\"\"\n",
        "    ext = Path(path).suffix.lower()\n",
        "    if ext == '.wav':\n",
        "        return read_wav_via_wave(path)\n",
        "    # try pydub as a fallback for formats like m4a, mp3\n",
        "    try:\n",
        "        from pydub import AudioSegment\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\n",
        "            f\"Cannot read '{path}': non-WAV audio requires pydub and ffmpeg/avlib.\\n\"\n",
        "            \"In Colab run in a separate cell: `!pip install pydub` and make sure ffmpeg is available.`\"\n",
        "        )\n",
        "    audio = AudioSegment.from_file(path)\n",
        "    audio = audio.set_frame_rate(16000).set_channels(1)\n",
        "    samples = np.array(audio.get_array_of_samples())\n",
        "    if audio.sample_width == 2:\n",
        "        samples = samples.astype(np.int16).astype(np.float32) / 32767.0\n",
        "    elif audio.sample_width == 4:\n",
        "        samples = samples.astype(np.int32).astype(np.float32) / 2147483647.0\n",
        "    else:\n",
        "        # generic normalization\n",
        "        maxv = float(np.abs(samples).max() or 1.0)\n",
        "        samples = samples.astype(np.float32) / maxv\n",
        "    return samples, audio.frame_rate\n",
        "\n",
        "\n",
        "def write_wav_via_wave(path: str, samples: np.ndarray, sr: int):\n",
        "    \"\"\"Write a mono float[-1..1] numpy array to WAV (16-bit) using wave module.\"\"\"\n",
        "    int16 = (np.clip(samples, -1.0, 1.0) * 32767).astype(np.int16)\n",
        "    with contextlib.closing(wave.open(path, 'wb')) as wf:\n",
        "        wf.setnchannels(1)\n",
        "        wf.setsampwidth(2)\n",
        "        wf.setframerate(sr)\n",
        "        wf.writeframes(int16.tobytes())\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# VAD (use webrtcvad if available, fallback to energy-based VAD)\n",
        "# --------------------\n",
        "\n",
        "def vad_segmenter(wav: np.ndarray, sr: int, aggressiveness: int = 2, frame_ms: int = 30) -> List[Tuple[float, float]]:\n",
        "    \"\"\"Return list of (start_s, end_s) voice-active segments.\n",
        "\n",
        "    Uses webrtcvad when available (recommended). If not present, falls back\n",
        "    to a simple energy-based VAD that is robust enough for noisy / test data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import webrtcvad\n",
        "        vad = webrtcvad.Vad(aggressiveness)\n",
        "        samples_per_frame = int(sr * frame_ms / 1000)\n",
        "        # convert float PCM (-1..1) to 16-bit PCM bytes\n",
        "        int16 = (wav * 32767).astype('int16')\n",
        "        bytes_wav = int16.tobytes()\n",
        "        segments = []\n",
        "        voiced = False\n",
        "        seg_start = 0.0\n",
        "        frame_byte_len = samples_per_frame * 2\n",
        "        total_frames = max(0, len(bytes_wav) // frame_byte_len)\n",
        "        for i in range(total_frames):\n",
        "            start_byte = i * frame_byte_len\n",
        "            frame = bytes_wav[start_byte:start_byte + frame_byte_len]\n",
        "            is_speech = vad.is_speech(frame, sr)\n",
        "            t = (i * samples_per_frame) / sr\n",
        "            if is_speech and not voiced:\n",
        "                voiced = True\n",
        "                seg_start = t\n",
        "            elif not is_speech and voiced:\n",
        "                voiced = False\n",
        "                seg_end = t\n",
        "                if seg_end - seg_start > 0.1:\n",
        "                    segments.append((seg_start, seg_end))\n",
        "        if voiced:\n",
        "            segments.append((seg_start, (len(wav) / sr)))\n",
        "        return segments\n",
        "    except Exception:\n",
        "        # Fallback energy-based VAD\n",
        "        samples_per_frame = int(sr * frame_ms / 1000)\n",
        "        if samples_per_frame <= 0:\n",
        "            return []\n",
        "        energies = []\n",
        "        for start in range(0, len(wav), samples_per_frame):\n",
        "            frame = wav[start:start + samples_per_frame]\n",
        "            energies.append(float(np.mean(np.abs(frame))) if frame.size else 0.0)\n",
        "        max_energy = max(energies) if energies else 0.0\n",
        "        thresh = max_energy * 0.15\n",
        "        voiced_flags = [e > thresh for e in energies]\n",
        "        segments = []\n",
        "        voiced = False\n",
        "        seg_start = 0.0\n",
        "        for i, flag in enumerate(voiced_flags):\n",
        "            t = (i * samples_per_frame) / sr\n",
        "            if flag and not voiced:\n",
        "                voiced = True\n",
        "                seg_start = t\n",
        "            elif not flag and voiced:\n",
        "                voiced = False\n",
        "                seg_end = t\n",
        "                if seg_end - seg_start > 0.1:\n",
        "                    segments.append((seg_start, seg_end))\n",
        "        if voiced:\n",
        "            segments.append((seg_start, len(wav) / sr))\n",
        "        return segments\n",
        "\n",
        "\n",
        "def merge_close_segments(segments: List[Tuple[float, float]], gap_thresh: float = 0.25) -> List[Tuple[float, float]]:\n",
        "    if not segments:\n",
        "        return []\n",
        "    merged = [list(segments[0])]\n",
        "    for s, e in segments[1:]:\n",
        "        if s - merged[-1][1] <= gap_thresh:\n",
        "            merged[-1][1] = e\n",
        "        else:\n",
        "            merged.append([s, e])\n",
        "    return [(a, b) for a, b in merged]\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# Speaker embedding extraction (lazy, optional)\n",
        "# --------------------\n",
        "\n",
        "def extract_speaker_segments(audio_path: str, segments: List[Tuple[float, float]], sr: int = 16000):\n",
        "    \"\"\"Extract per-segment temporary WAVs and compute embeddings if resemblyzer is available.\n",
        "\n",
        "    Returns (embeddings_or_None, seg_wavs)\n",
        "      - embeddings_or_None: np.ndarray of shape (n_segments, dim) or None if embeddings unavailable\n",
        "      - seg_wavs: list of (start, end, tmpfile)\n",
        "    \"\"\"\n",
        "    seg_wavs = []\n",
        "    # Make temp directory\n",
        "    tmpdir = tempfile.mkdtemp(prefix='cqa_')\n",
        "    # try to load the full audio samples so we can slice directly\n",
        "    try:\n",
        "        samples, sr_file = load_audio_mono(audio_path)\n",
        "    except Exception as e:\n",
        "        print('extract_speaker_segments: failed to load audio:', e)\n",
        "        return None, seg_wavs\n",
        "    for idx, (s, e) in enumerate(segments):\n",
        "        a = int(max(0, int(s * sr_file)))\n",
        "        b = int(min(len(samples), int(e * sr_file)))\n",
        "        seg_samples = samples[a:b]\n",
        "        tmp = os.path.join(tmpdir, f'temp_seg_{idx}.wav')\n",
        "        write_wav_via_wave(tmp, seg_samples, sr_file)\n",
        "        seg_wavs.append((s, e, tmp))\n",
        "    # try to compute embeddings using resemblyzer\n",
        "    try:\n",
        "        from resemblyzer import VoiceEncoder, preprocess_wav\n",
        "        encoder = VoiceEncoder()\n",
        "        embeddings = []\n",
        "        for (_, _, tmp) in seg_wavs:\n",
        "            wav_pre = preprocess_wav(tmp)\n",
        "            emb = encoder.embed_utterance(wav_pre)\n",
        "            embeddings.append(emb)\n",
        "        if not embeddings:\n",
        "            return None, seg_wavs\n",
        "        return np.vstack(embeddings), seg_wavs\n",
        "    except Exception:\n",
        "        # Embeddings unavailable in this environment; return None to signal fallback.\n",
        "        print('extract_speaker_segments: resemblyzer not available — skipping embeddings.')\n",
        "        return None, seg_wavs\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# Transcription helpers (lazy)\n",
        "# --------------------\n",
        "\n",
        "def transcribe_with_openai(audio_path: str, openai_api_key: str):\n",
        "    try:\n",
        "        from openai import OpenAI\n",
        "    except Exception:\n",
        "        raise RuntimeError('OpenAI package not installed; set up faster-whisper or install openai package.')\n",
        "    client = OpenAI(api_key=openai_api_key)\n",
        "    with open(audio_path, 'rb') as f:\n",
        "        res = client.audio.transcriptions.create(model='whisper-1', file=f)\n",
        "    text = res.text if hasattr(res, 'text') else res['text']\n",
        "    return text, []\n",
        "\n",
        "\n",
        "def transcribe_with_whisper(audio_path: str, lang: str = 'en'):\n",
        "    try:\n",
        "        from faster_whisper import WhisperModel\n",
        "    except Exception:\n",
        "        raise RuntimeError('faster-whisper is not installed in this environment.')\n",
        "    model_size = 'small'\n",
        "    model = WhisperModel(model_size, device='cpu', compute_type='int8_float16')\n",
        "    segments, info = model.transcribe(audio_path, beam_size=5, language=lang)\n",
        "    transcript = ''\n",
        "    segs = []\n",
        "    for segment in segments:\n",
        "        transcript += segment.text + ' '\n",
        "        segs.append({'start': segment.start, 'end': segment.end, 'text': segment.text})\n",
        "    return transcript.strip(), segs\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# Small helpers (testable units)\n",
        "# --------------------\n",
        "\n",
        "def compute_talktime(timeline: List[dict]) -> Tuple[dict, dict]:\n",
        "    \"\"\"Given timeline list [{'start':s,'end':e,'label':l}, ...], returns talktime dict and ratio.\"\"\"\n",
        "    talktime = {}\n",
        "    for item in timeline:\n",
        "        dur = item['end'] - item['start']\n",
        "        talktime[item['label']] = talktime.get(item['label'], 0) + dur\n",
        "    total_spoken = sum(talktime.values()) or 1.0\n",
        "    talktime_ratio = {lab: (dur / total_spoken) * 100 for lab, dur in talktime.items()}\n",
        "    return talktime, talktime_ratio\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# High-level pipeline function\n",
        "# --------------------\n",
        "\n",
        "def analyze_call(youtube_url: str, openai_api_key: Optional[str] = None, max_speaker: int = 2) -> dict:\n",
        "    t0 = time.time()\n",
        "    audio_path = 'call.wav'\n",
        "\n",
        "    # Download + convert audio (uses yt-dlp + pydub). These are heavy dependencies\n",
        "    # and should be installed in Colab in a separate cell.\n",
        "    print('Downloading audio... (requires yt-dlp and pydub to be installed in Colab)')\n",
        "    try:\n",
        "        # use yt-dlp to fetch the audio file then pydub to convert to WAV\n",
        "        import shlex\n",
        "        tmpfile = 'yt_audio.m4a'\n",
        "        cmd = f\"yt-dlp -x --audio-format m4a -o '{tmpfile}' '{youtube_url}'\"\n",
        "        subprocess.run(shlex.split(cmd), check=True)\n",
        "        # convert to mono 16k wav using pydub\n",
        "        try:\n",
        "            from pydub import AudioSegment\n",
        "        except Exception:\n",
        "            raise RuntimeError('pydub not installed; install it in Colab (see top-of-file notes).')\n",
        "        audio = AudioSegment.from_file(tmpfile)\n",
        "        audio = audio.set_frame_rate(16000).set_channels(1)\n",
        "        audio.export(audio_path, format='wav')\n",
        "        try:\n",
        "            os.remove(tmpfile)\n",
        "        except Exception:\n",
        "            pass\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f'Failed to download or convert audio: {e}')\n",
        "\n",
        "    # Load audio samples without soundfile\n",
        "    wav, sr = load_audio_mono(audio_path)\n",
        "\n",
        "    # VAD -> segments\n",
        "    print('Running VAD...')\n",
        "    segments = vad_segmenter(wav, sr)\n",
        "    segments = merge_close_segments(segments, gap_thresh=0.3)\n",
        "    if not segments:\n",
        "        raise RuntimeError('No speech detected.')\n",
        "    print(f'Detected {len(segments)} speech segments')\n",
        "\n",
        "    # Speaker embeddings (optional)\n",
        "    print('Extracting speaker embeddings (if available)...')\n",
        "    embeddings, seg_wavs = extract_speaker_segments(audio_path, segments, sr)\n",
        "    num_segments = len(seg_wavs)\n",
        "    # If embeddings unavailable, fall back to deterministic labels\n",
        "    labels = None\n",
        "    if embeddings is None or (isinstance(embeddings, np.ndarray) and embeddings.size == 0):\n",
        "        print('Embeddings unavailable — assigning labels by simple round-robin (fallback).')\n",
        "        labels = np.array([i % max_speaker for i in range(num_segments)], dtype=int)\n",
        "    else:\n",
        "        # try to cluster using scikit-learn KMeans\n",
        "        try:\n",
        "            from sklearn.cluster import KMeans\n",
        "            n_clusters = min(max_speaker, max(1, embeddings.shape[0]))\n",
        "            if n_clusters <= 1 or embeddings.shape[0] <= 1:\n",
        "                labels = np.zeros(embeddings.shape[0], dtype=int)\n",
        "            else:\n",
        "                kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(embeddings)\n",
        "                labels = kmeans.labels_\n",
        "        except Exception:\n",
        "            print('scikit-learn not available — using fallback labels.')\n",
        "            labels = np.array([i % max_speaker for i in range(num_segments)], dtype=int)\n",
        "\n",
        "    # build timeline\n",
        "    timeline = []\n",
        "    for i, (s, e, tmp) in enumerate(seg_wavs):\n",
        "        timeline.append({'start': s, 'end': e, 'label': int(labels[i])})\n",
        "\n",
        "    talktime, talktime_ratio = compute_talktime(timeline)\n",
        "    longest = max(timeline, key=lambda x: x['end'] - x['start'])\n",
        "    longest_duration = longest['end'] - longest['start']\n",
        "\n",
        "    # ASR\n",
        "    print('Transcribing (this may require faster-whisper or OpenAI and can be slow)...')\n",
        "    try:\n",
        "        if openai_api_key:\n",
        "            transcript, segs = transcribe_with_openai(audio_path, openai_api_key)\n",
        "        else:\n",
        "            transcript, segs = transcribe_with_whisper(audio_path)\n",
        "    except Exception as e:\n",
        "        print('ASR failed or is unavailable in this environment:', e)\n",
        "        transcript = ''\n",
        "        segs = []\n",
        "\n",
        "    # Question counting (simple heuristic)\n",
        "    q_words = set(['who', 'what', 'when', 'where', 'why', 'how', 'do', 'does', 'did', 'is', 'are', 'can', 'could', 'would', 'should'])\n",
        "    question_count = 0\n",
        "    sentences = re.split(r'[\\n\\.]+', transcript.lower()) if transcript else []\n",
        "    for s in sentences:\n",
        "        s = s.strip()\n",
        "        if not s:\n",
        "            continue\n",
        "        if s.endswith('?'):\n",
        "            question_count += 1\n",
        "        else:\n",
        "            first = s.split()[0] if s.split() else ''\n",
        "            if first in q_words:\n",
        "                question_count += 1\n",
        "\n",
        "    # Sentiment (best-effort)\n",
        "    sentiment = 'neutral'\n",
        "    try:\n",
        "        from transformers import pipeline\n",
        "        sentiment_pipeline = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "        if transcript:\n",
        "            sent = sentiment_pipeline(transcript[:1000])\n",
        "            label = sent[0]['label'].lower()\n",
        "            if 'positive' in label:\n",
        "                sentiment = 'positive'\n",
        "            elif 'negative' in label:\n",
        "                sentiment = 'negative'\n",
        "            else:\n",
        "                sentiment = 'neutral'\n",
        "    except Exception:\n",
        "        print('transformers not available — skipping sentiment analysis.')\n",
        "\n",
        "    # Actionable insight (simple rules)\n",
        "    insight = ''\n",
        "    speaker_max = max(talktime, key=talktime.get) if talktime else None\n",
        "    if speaker_max is not None and talktime_ratio.get(speaker_max, 0) > 70:\n",
        "        insight = 'Sales rep is dominating the call; allow customer more time to speak.'\n",
        "    elif question_count < 3:\n",
        "        insight = 'Sales rep asked too few questions; ask open-ended questions to discover needs.'\n",
        "    elif sentiment == 'negative':\n",
        "        insight = 'Customer sentiment is negative; address concerns and ask clarifying questions.'\n",
        "    else:\n",
        "        insight = 'Good balance — keep following up with clarifying questions.'\n",
        "\n",
        "    # Bonus: guess rep label by counting which speaker's segments contain questions\n",
        "    guessed_rep = None\n",
        "    label_question_counts = {i: 0 for i in range(max_speaker)}\n",
        "    for s in segs:\n",
        "        mid = (s['start'] + s['end']) / 2\n",
        "        for t in timeline:\n",
        "            if t['start'] <= mid <= t['end']:\n",
        "                words = re.findall(r\"\\w+\", s['text'].lower())\n",
        "                if any(w in q_words for w in words):\n",
        "                    label_question_counts[t['label']] += 1\n",
        "                break\n",
        "    if label_question_counts:\n",
        "        guessed_rep = max(label_question_counts, key=lambda k: label_question_counts[k])\n",
        "\n",
        "    outputs = {\n",
        "        'talktime_ratio': talktime_ratio,\n",
        "        'question_count': question_count,\n",
        "        'longest_monologue_sec': longest_duration,\n",
        "        'sentiment': sentiment,\n",
        "        'actionable_insight': insight,\n",
        "        'guessed_rep_label': int(guessed_rep) if guessed_rep is not None else None,\n",
        "        'timeline': timeline,\n",
        "        'transcript_excerpt': transcript[:200]\n",
        "    }\n",
        "    outputs['elapsed_sec'] = time.time() - t0\n",
        "    return outputs\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# Unit tests (lightweight, no heavy deps needed)\n",
        "# --------------------\n",
        "\n",
        "def test_merge_close_segments():\n",
        "    segments = [(0.0, 0.1), (0.15, 0.3), (0.9, 1.0)]\n",
        "    merged = merge_close_segments(segments, gap_thresh=0.2)\n",
        "    assert merged == [(0.0, 0.3), (0.9, 1.0)], f'merge_close_segments failed: {merged}'\n",
        "    print('test_merge_close_segments: OK')\n",
        "\n",
        "\n",
        "def test_compute_talktime():\n",
        "    timeline = [\n",
        "        {'start': 0.0, 'end': 1.0, 'label': 0},\n",
        "        {'start': 1.0, 'end': 2.5, 'label': 1},\n",
        "        {'start': 2.5, 'end': 3.5, 'label': 0},\n",
        "    ]\n",
        "    talktime, ratio = compute_talktime(timeline)\n",
        "    assert round(talktime[0], 3) == 2.0\n",
        "    assert round(talktime[1], 3) == 1.5\n",
        "    assert abs(ratio[0] - (2.0 / 3.5 * 100)) < 0.001\n",
        "    print('test_compute_talktime: OK')\n",
        "\n",
        "\n",
        "def run_unit_tests():\n",
        "    test_merge_close_segments()\n",
        "    test_compute_talktime()\n",
        "    print('All lightweight tests passed.')\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# 7) Run example / tests (preserve original test cases + extras)\n",
        "# --------------------\n",
        "if __name__ == '__main__':\n",
        "    # Run lightweight unit tests first (these don't require heavy audio libs)\n",
        "    run_unit_tests()\n",
        "\n",
        "    # Original provided test (will require external deps installed):\n",
        "    YT = 'https://www.youtube.com/watch?v=4ostqJD3Psc'\n",
        "    OPENAI_KEY = os.environ.get('OPENAI_API_KEY', None)\n",
        "\n",
        "    # NOTE: If you haven't installed the dependencies, either:\n",
        "    #  - Run the Colab pip command in a separate code cell (recommended), or\n",
        "    #  - Set AUTO_INSTALL_DEPENDENCIES=1 and call maybe_install_packages() before running.\n",
        "    # Example (uncomment to auto-install in a controlled manner):\n",
        "    # os.environ['AUTO_INSTALL_DEPENDENCIES'] = '1'\n",
        "    # maybe_install_packages()\n",
        "\n",
        "    try:\n",
        "        print('\\nRunning analyze_call() on test YouTube audio (this requires dependencies)...')\n",
        "        result = analyze_call(YT, openai_api_key=OPENAI_KEY)\n",
        "        import json\n",
        "        print(json.dumps(result, indent=2))\n",
        "    except Exception as e:\n",
        "        print('analyze_call failed (likely missing packages or runtime error):', e)\n",
        "\n",
        "    # Extra test case: ensure function raises RuntimeError for empty audio via VAD\n",
        "    try:\n",
        "        empty_path = 'empty.wav'\n",
        "        # create 1 second of silence at 16kHz\n",
        "        sr = 16000\n",
        "        silence = np.zeros(sr, dtype=np.float32)\n",
        "        write_wav_via_wave(empty_path, silence, sr)\n",
        "        wav, sr_loaded = load_audio_mono(empty_path)\n",
        "        segs = vad_segmenter(wav, sr_loaded)\n",
        "        if not segs:\n",
        "            raise RuntimeError('No speech detected.')\n",
        "    except RuntimeError as e:\n",
        "        print('Expected test case (empty audio):', e)\n",
        "\n",
        "    print('\\nDone.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uqc0UoM3LgVF",
        "outputId": "9701c058-c7bd-4e81-e06d-82649d1075af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}
